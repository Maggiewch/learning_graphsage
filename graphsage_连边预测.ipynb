{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from graphsage.encoders import Encoder\n",
    "from graphsage.aggregators import MeanAggregator\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "from sklearn.metrics import f1_score\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "from graphsage.encoders import Encoder\n",
    "from graphsage.aggregators import MeanAggregator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Simple supervised GraphSAGE model as well as examples running the model\n",
    "on the Cora and Pubmed datasets.\n",
    "\"\"\"\n",
    "class SupervisedGraphSage(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes, enc):\n",
    "        super(SupervisedGraphSage, self).__init__()\n",
    "        self.enc = enc\n",
    "        self.xent = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(num_classes, enc.embed_dim))\n",
    "        init.xavier_uniform(self.weight)\n",
    "\n",
    "    def forward(self, nodes):\n",
    "        embeds = self.enc(nodes)\n",
    "        scores = self.weight.mm(embeds)\n",
    "        return scores.t()\n",
    "\n",
    "    def loss(self, nodes, labels):\n",
    "        scores = self.forward(nodes)\n",
    "        return self.xent(scores, labels.squeeze())\n",
    "\n",
    "def load_cora():\n",
    "    num_nodes = 2708\n",
    "    num_feats = 1433\n",
    "    feat_data = np.zeros((num_nodes, num_feats))\n",
    "    labels = np.empty((num_nodes,1), dtype=np.int64)\n",
    "    node_map = {}\n",
    "    label_map = {}\n",
    "    with open(\"cora/cora.content\") as fp:\n",
    "        for i,line in enumerate(fp):\n",
    "            info = line.strip().split()\n",
    "            feat_data[i,:] = [float(info[i]) for i in range(1,len(info)-1)]\n",
    "            node_map[info[0]] = i\n",
    "            if not info[-1] in label_map:\n",
    "                label_map[info[-1]] = len(label_map)\n",
    "            labels[i] = label_map[info[-1]]\n",
    "\n",
    "    adj_lists = defaultdict(set)\n",
    "    with open(\"cora/cora.cites\") as fp:\n",
    "        for i,line in enumerate(fp):\n",
    "            info = line.strip().split()\n",
    "            paper1 = node_map[info[0]]\n",
    "            paper2 = node_map[info[1]]\n",
    "            adj_lists[paper1].add(paper2)\n",
    "            adj_lists[paper2].add(paper1)\n",
    "    return feat_data, labels, adj_lists, node_map\n",
    "\n",
    "def run_cora():\n",
    "    np.random.seed(1)\n",
    "    random.seed(1)\n",
    "    num_nodes = 2708\n",
    "    feat_data, labels, adj_lists = load_cora()\n",
    "    features = nn.Embedding(2708, 1433)\n",
    "    features.weight = nn.Parameter(torch.FloatTensor(feat_data), requires_grad=False)\n",
    "   # features.cuda()\n",
    "\n",
    "    agg1 = MeanAggregator(features, cuda=True)\n",
    "    enc1 = Encoder(features, 1433, 128, adj_lists, agg1, gcn=True, cuda=False)\n",
    "    agg2 = MeanAggregator(lambda nodes : enc1(nodes).t(), cuda=False)\n",
    "    enc2 = Encoder(lambda nodes : enc1(nodes).t(), enc1.embed_dim, 128, adj_lists, agg2,\n",
    "            base_model=enc1, gcn=True, cuda=False)\n",
    "    enc1.num_samples = 5\n",
    "    enc2.num_samples = 5\n",
    "\n",
    "    graphsage = SupervisedGraphSage(7, enc2)\n",
    "    #graphsage.cuda()\n",
    "    rand_indices = np.random.permutation(num_nodes)\n",
    "    test = rand_indices[:1000]\n",
    "    val = rand_indices[1000:1500]\n",
    "    train = list(rand_indices[1500:])\n",
    "\n",
    "    optimizer = torch.optim.SGD(filter(lambda p : p.requires_grad, graphsage.parameters()), lr=0.7)\n",
    "    times = []\n",
    "    for batch in range(100):\n",
    "        batch_nodes = train[:256]\n",
    "        random.shuffle(train)\n",
    "        start_time = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        loss = graphsage.loss(batch_nodes, \n",
    "                Variable(torch.LongTensor(labels[np.array(batch_nodes)])))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        end_time = time.time()\n",
    "        times.append(end_time-start_time)\n",
    "        print (batch, loss.data[0])\n",
    "\n",
    "    val_output = graphsage.forward(val) \n",
    "    print (\"Validation F1:\", f1_score(labels[val], val_output.data.numpy().argmax(axis=1), average=\"micro\"))\n",
    "    print (\"Average batch time:\", np.mean(times))\n",
    "\n",
    "def load_pubmed():\n",
    "    #hardcoded for simplicity...\n",
    "    num_nodes = 19717\n",
    "    num_feats = 500\n",
    "    feat_data = np.zeros((num_nodes, num_feats))\n",
    "    labels = np.empty((num_nodes, 1), dtype=np.int64)\n",
    "    node_map = {}\n",
    "    with open(\"pubmed-data/Pubmed-Diabetes.NODE.paper.tab\") as fp:\n",
    "        fp.readline()\n",
    "        feat_map = {entry.split(\":\")[1]:i-1 for i,entry in enumerate(fp.readline().split(\"\\t\"))}\n",
    "        for i, line in enumerate(fp):\n",
    "            info = line.split(\"\\t\")\n",
    "            node_map[info[0]] = i\n",
    "            labels[i] = int(info[1].split(\"=\")[1])-1\n",
    "            for word_info in info[2:-1]:\n",
    "                word_info = word_info.split(\"=\")\n",
    "                feat_data[i][feat_map[word_info[0]]] = float(word_info[1])\n",
    "    adj_lists = defaultdict(set)\n",
    "    with open(\"pubmed-data/Pubmed-Diabetes.DIRECTED.cites.tab\") as fp:\n",
    "        fp.readline()\n",
    "        fp.readline()\n",
    "        for line in fp:\n",
    "            info = line.strip().split(\"\\t\")\n",
    "            paper1 = node_map[info[1].split(\":\")[1]]\n",
    "            paper2 = node_map[info[-1].split(\":\")[1]]\n",
    "            adj_lists[paper1].add(paper2)\n",
    "            adj_lists[paper2].add(paper1)\n",
    "    return feat_data, labels, adj_lists\n",
    "\n",
    "def run_pubmed():\n",
    "    np.random.seed(1)\n",
    "    random.seed(1)\n",
    "    num_nodes = 19717\n",
    "    feat_data, labels, adj_lists = load_pubmed()\n",
    "    features = nn.Embedding(19717, 500)\n",
    "    features.weight = nn.Parameter(torch.FloatTensor(feat_data), requires_grad=False)\n",
    "   # features.cuda()\n",
    "\n",
    "    agg1 = MeanAggregator(features, cuda=True)\n",
    "    enc1 = Encoder(features, 500, 128, adj_lists, agg1, gcn=True, cuda=False)\n",
    "    agg2 = MeanAggregator(lambda nodes : enc1(nodes).t(), cuda=False)\n",
    "    enc2 = Encoder(lambda nodes : enc1(nodes).t(), enc1.embed_dim, 128, adj_lists, agg2,\n",
    "            base_model=enc1, gcn=True, cuda=False)\n",
    "    enc1.num_samples = 10\n",
    "    enc2.num_samples = 25\n",
    "\n",
    "    graphsage = SupervisedGraphSage(3, enc2)\n",
    "    #graphsage.cuda()\n",
    "    rand_indices = np.random.permutation(num_nodes)\n",
    "    test = rand_indices[:1000]\n",
    "    val = rand_indices[1000:1500]\n",
    "    train = list(rand_indices[1500:])\n",
    "\n",
    "    optimizer = torch.optim.SGD(filter(lambda p : p.requires_grad, graphsage.parameters()), lr=0.7)\n",
    "    times = []\n",
    "    for batch in range(200):\n",
    "        batch_nodes = train[:1024]\n",
    "        random.shuffle(train)\n",
    "        start_time = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        loss = graphsage.loss(batch_nodes, \n",
    "                Variable(torch.LongTensor(labels[np.array(batch_nodes)])))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        end_time = time.time()\n",
    "        times.append(end_time-start_time)\n",
    "        print (batch, loss.data[0])\n",
    "\n",
    "    val_output = graphsage.forward(val) \n",
    "    print (\"Validation F1:\", f1_score(labels[val], val_output.data.numpy().argmax(axis=1), average=\"micro\"))\n",
    "    print (\"Average batch time:\", np.mean(times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feat_data, labels, adj_lists, nodes_map = load_cora()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据准备——所有节点与163号节点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "node_list = []\n",
    "source_list = []\n",
    "target_list = []\n",
    "for node in nodes_map:\n",
    "    node_list.append(nodes_map[node])\n",
    "    for end in adj_lists[nodes_map[node]]:\n",
    "        source_list.append(nodes_map[node])\n",
    "        target_list.append(end)\n",
    "data_nolink = []\n",
    "for node in nodes_map:\n",
    "    for in_node in node_list:\n",
    "        if in_node not in adj_lists[nodes_map[node]]:\n",
    "            data_nolink.append([nodes_map[node], in_node])\n",
    "data_link = np.array([source_list,target_list]).T            \n",
    "data_nolink = np.array(data_nolink)\n",
    "rand_indices = np.random.permutation(len(data_nolink))\n",
    "all_sample = np.vstack((data_link,data_nolink[rand_indices[:len(data_link)]]))\n",
    "\n",
    "#len(all_sample)\n",
    "\n",
    "linkprediction_label = []\n",
    "for i in range(len(all_sample)):\n",
    "    if i < len(data_link):\n",
    "        linkprediction_label.append([1])\n",
    "    else:\n",
    "        linkprediction_label.append([0])\n",
    "\n",
    "\n",
    "linkprediction_label = np.array(linkprediction_label)\n",
    "linkprediction_label_bcel = torch.FloatTensor(linkprediction_label)\n",
    "\n",
    "#all_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2540\n"
     ]
    }
   ],
   "source": [
    "adj_163 = list(adj_lists[163])\n",
    "no_adj_163 = []\n",
    "for node in node_list:\n",
    "    if node not in adj_163:\n",
    "        no_adj_163.append(node)\n",
    "print(len(no_adj_163))\n",
    "\n",
    "rand_index = np.random.permutation(len(no_adj_163))\n",
    "sample_no_adj = rand_index[:len(adj_163)]\n",
    "sample_index = copy.copy(adj_163)\n",
    "sample_index.extend(sample_no_adj)\n",
    "sample_rand_index = np.array([sample_index[i] for i in np.random.permutation(len(sample_index))])\n",
    "\n",
    "labels_163 = []\n",
    "for node in node_list :\n",
    "    if node in adj_163:\n",
    "        labels_163.append(1)\n",
    "    else:\n",
    "        labels_163.append(0)\n",
    "        \n",
    "all_sample_163 = copy.copy(adj_163)\n",
    "all_sample_163.extend(no_adj_163)\n",
    "\n",
    "node_prediction_163 = []\n",
    "for la in labels_163:\n",
    "    node_prediction_163.append([la])\n",
    "\n",
    "node_prediction_163 = np.array(node_prediction_163)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "node_prediction_163_bcel = torch.FloatTensor(node_prediction_163)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验一： 163号节点连边的预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SupervisedGraphSage(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes, enc):\n",
    "        super(SupervisedGraphSage, self).__init__()\n",
    "        self.enc = enc\n",
    "        self.xent = nn.BCELoss()\n",
    "\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(num_classes, enc.embed_dim))\n",
    "        init.xavier_uniform(self.weight)\n",
    "\n",
    "    def forward(self, nodes1):\n",
    "        embeds1 = self.enc(nodes1)\n",
    "        #embeds2 = self.enc(nodes2)\n",
    "        print(embeds1.shape)\n",
    "        #print(embeds2.shape)\n",
    "        print(self.weight.shape)\n",
    "        #print ((embeds1*embeds2).shape)#inner directly\n",
    "        scores = self.weight.mm(embeds1)\n",
    "        #print(scores)\n",
    "        \n",
    "        return scores.t()\n",
    "\n",
    "    def loss(self, nodes1, labels):\n",
    "        scores = self.forward(nodes1)\n",
    "        scores = F.sigmoid(scores)\n",
    "        return self.xent(scores, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "num_nodes = 2708\n",
    "feat_data, labels, adj_lists,_ = load_cora()\n",
    "features = nn.Embedding(2708, 1433)\n",
    "features.weight = nn.Parameter(torch.FloatTensor(feat_data), requires_grad=False)\n",
    "# features.cuda()\n",
    "\n",
    "agg1 = MeanAggregator(features, cuda=True)\n",
    "enc1 = Encoder(features, 1433, 128, adj_lists, agg1, gcn=True, cuda=False)\n",
    "agg2 = MeanAggregator(lambda nodes : enc1(nodes).t(), cuda=False)\n",
    "enc2 = Encoder(lambda nodes : enc1(nodes).t(), enc1.embed_dim, 1433, adj_lists, agg2,\n",
    "        base_model=enc1, gcn=True, cuda=False)\n",
    "enc1.num_samples = 5\n",
    "enc2.num_samples = 5\n",
    "#这里改一下输出的维度\n",
    "graphsage = SupervisedGraphSage(1, enc2)\n",
    "#graphsage.cuda()\n",
    "#rand_indices = np.random.permutation(len(node_list))\n",
    "test = sample_rand_index[:50]\n",
    "val = sample_rand_index[50:100]\n",
    "train = list(sample_rand_index[100:])\n",
    "\n",
    "optimizer = torch.optim.SGD(filter(lambda p : p.requires_grad, graphsage.parameters()), lr=0.5)\n",
    "times = []\n",
    "Loss = []\n",
    "for batch in range(200):\n",
    "    #batch_nodes1 = np.ones(100)*163#No.163\n",
    "    batch_nodes2 = train[:40]\n",
    "    batch_idx = copy.copy(train[:40])\n",
    "    random.shuffle(train)\n",
    "    start_time = time.time()\n",
    "    optimizer.zero_grad()\n",
    "    loss = graphsage.loss(batch_nodes2,\n",
    "            Variable(node_prediction_163_bcel[np.array(batch_idx)]))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    end_time = time.time()\n",
    "    times.append(end_time-start_time)\n",
    "    #print (batch, loss.data[0])\n",
    "    Loss.append(loss.data[0])\n",
    "val_output = graphsage.forward(val) \n",
    "#print (\"Validation F1:\", f1_score(node_prediction_163_bcel[val], val_output.data.numpy().argmax(axis=1), average=\"micro\"))\n",
    "#print (\"Average batch time:\", np.mean(times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "right = []\n",
    "wrong = []\n",
    "predict_list = F.sigmoid(val_output)\n",
    "for i in range(len(F.sigmoid(val_output))):\n",
    "    predict = predict_list[i]\n",
    "    target = node_prediction_163_bcel[val][i]\n",
    "    print(abs(predict - target))\n",
    "    if abs(predict - target)<0.5:\n",
    "        right.append(i)\n",
    "    else:\n",
    "        wrong.append(i)\n",
    "print(len(right)/(len(right)+len(wrong)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验二 节点对的连边预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SupervisedGraphSage(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes, enc):\n",
    "        super(SupervisedGraphSage, self).__init__()\n",
    "        self.enc = enc\n",
    "        self.xent = nn.BCELoss()\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(num_classes, enc.embed_dim))\n",
    "        init.xavier_uniform(self.weight)\n",
    "\n",
    "    def forward(self, nodes1, nodes2):\n",
    "        embeds1 = self.enc(nodes1)\n",
    "        embeds2 = self.enc(nodes2)\n",
    "        print(embeds1.shape)\n",
    "        #print(embeds2.shape)\n",
    "        print(self.weight.shape)\n",
    "        #print ((embeds1*embeds2).shape)#inner directly\n",
    "        scores = (torch.ones((1,self.enc.embed_dim))).mm(embeds1*embeds2)\n",
    "        #print(scores)\n",
    "        \n",
    "        return scores.t()\n",
    "\n",
    "    def loss(self, nodes1, nodes2, labels):\n",
    "        scores = self.forward(nodes1,nodes2)\n",
    "        scores = F.sigmoid(scores)\n",
    "        return self.xent(scores, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "num_nodes = 2708\n",
    "feat_data, labels, adj_lists,_ = load_cora()\n",
    "features = nn.Embedding(2708, 1433)\n",
    "features.weight = nn.Parameter(torch.FloatTensor(feat_data), requires_grad=False)\n",
    "# features.cuda()\n",
    "\n",
    "agg1 = MeanAggregator(features, cuda=True)\n",
    "enc1 = Encoder(features, 1433, 128, adj_lists, agg1, gcn=True, cuda=False)\n",
    "agg2 = MeanAggregator(lambda nodes : enc1(nodes).t(), cuda=False)\n",
    "enc2 = Encoder(lambda nodes : enc1(nodes).t(), enc1.embed_dim, 128, adj_lists, agg2,\n",
    "        base_model=enc1, gcn=True, cuda=False)\n",
    "enc1.num_samples = 5\n",
    "enc2.num_samples = 5\n",
    "\n",
    "graphsage = SupervisedGraphSage(1, enc2)\n",
    "#graphsage.cuda()\n",
    "rand_indices = np.random.permutation(len(all_sample))\n",
    "test = rand_indices[:1000]\n",
    "val = rand_indices[1000:1500]\n",
    "train = list(rand_indices[1500:])\n",
    "\n",
    "optimizer = torch.optim.SGD(filter(lambda p : p.requires_grad, graphsage.parameters()), lr=0.01)\n",
    "times = []\n",
    "Loss = []\n",
    "for batch in range(100):\n",
    "    batch_nodes1 = all_sample[train[:32]][:,0]\n",
    "    batch_nodes2 = all_sample[train[:32]][:,1]\n",
    "    batch_idx = copy.copy(train[:32])\n",
    "    random.shuffle(train)\n",
    "    start_time = time.time()\n",
    "    optimizer.zero_grad()\n",
    "    loss = graphsage.loss(batch_nodes1, batch_nodes2,\n",
    "            Variable(linkprediction_label_bcel[np.array(batch_idx)]))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    end_time = time.time()\n",
    "    times.append(end_time-start_time)\n",
    "    print (batch, loss.data[0])\n",
    "    Loss.append(loss.data[0])\n",
    "val_output = graphsage.forward(all_sample[val][:,0],all_sample[val][:,1]) \n",
    "print (\"Validation F1:\", f1_score(linkprediction_label_bcel[val], val_output.data.numpy().argmax(axis=1), average=\"micro\"))\n",
    "print (\"Average batch time:\", np.mean(times))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验三 两个节点拼接，预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SupervisedGraphSage(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes, enc):\n",
    "        super(SupervisedGraphSage, self).__init__()\n",
    "        self.enc = enc\n",
    "        self.xent = nn.BCELoss()\n",
    "\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(num_classes, enc.embed_dim*2))\n",
    "        init.xavier_uniform(self.weight)\n",
    "\n",
    "    def forward(self, nodes1, nodes2):\n",
    "        embeds1 = self.enc(nodes1)\n",
    "        embeds2 = self.enc(nodes2)\n",
    "        print(embeds1.shape)\n",
    "        #print(embeds2.shape)\n",
    "        print(self.weight.shape)\n",
    "        #print ((embeds1*embeds2).shape)#inner directly\n",
    "        combined = torch.cat([embeds1, embeds2], dim=0)\n",
    "        scores = self.weight.mm(combined)\n",
    "        #print(scores)\n",
    "        \n",
    "        return scores.t()\n",
    "\n",
    "    def loss(self, nodes1, nodes2, labels):\n",
    "        scores = self.forward(nodes1,nodes2)\n",
    "        scores = F.sigmoid(scores)\n",
    "        return self.xent(scores, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "num_nodes = 2708\n",
    "feat_data, labels, adj_lists,_ = load_cora()\n",
    "features = nn.Embedding(2708, 1433)\n",
    "features.weight = nn.Parameter(torch.FloatTensor(feat_data), requires_grad=False)\n",
    "# features.cuda()\n",
    "\n",
    "agg1 = MeanAggregator(features, cuda=True)\n",
    "enc1 = Encoder(features, 1433, 128, adj_lists, agg1, gcn=True, cuda=False)\n",
    "agg2 = MeanAggregator(lambda nodes : enc1(nodes).t(), cuda=False)\n",
    "enc2 = Encoder(lambda nodes : enc1(nodes).t(), enc1.embed_dim, 128, adj_lists, agg2,\n",
    "        base_model=enc1, gcn=True, cuda=False)\n",
    "enc1.num_samples = 5\n",
    "enc2.num_samples = 5\n",
    "\n",
    "graphsage = SupervisedGraphSage(1, enc2)\n",
    "#graphsage.cuda()\n",
    "rand_indices = np.random.permutation(len(all_sample))\n",
    "test = rand_indices[:1000]\n",
    "val = rand_indices[1000:1500]\n",
    "train = list(rand_indices[1500:])\n",
    "\n",
    "optimizer = torch.optim.SGD(filter(lambda p : p.requires_grad, graphsage.parameters()), lr=0.5)\n",
    "times = []\n",
    "Loss = []\n",
    "for batch in range(500):\n",
    "    batch_nodes1 = all_sample[train[:256]][:,0]\n",
    "    batch_nodes2 = all_sample[train[:256]][:,1]\n",
    "    batch_idx = copy.copy(train[:256])\n",
    "    random.shuffle(train)\n",
    "    start_time = time.time()\n",
    "    optimizer.zero_grad()\n",
    "    loss = graphsage.loss(batch_nodes1, batch_nodes2,\n",
    "            Variable(linkprediction_label_bcel[np.array(batch_idx)]))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    end_time = time.time()\n",
    "    times.append(end_time-start_time)\n",
    "    print (batch, loss.data[0])\n",
    "    Loss.append(loss.data[0])\n",
    "val_output = graphsage.forward(all_sample[val][:,0],all_sample[val][:,1]) \n",
    "print (\"Validation F1:\", f1_score(linkprediction_label_bcel[val], val_output.data.numpy().argmax(axis=1), average=\"micro\"))\n",
    "print (\"Average batch time:\", np.mean(times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(Loss)\n",
    "plt.xlabel('batches')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "right = []\n",
    "wrong = []\n",
    "predict_list = F.sigmoid(val_output)\n",
    "for i in range(len(F.sigmoid(val_output))):\n",
    "    predict = predict_list[i]\n",
    "    target = linkprediction_label_bcel[val][i]\n",
    "    print(abs(predict - target))\n",
    "    if abs(predict - target)<0.5:\n",
    "        right.append(i)\n",
    "    else:\n",
    "        wrong.append(i)\n",
    "print(len(right)/(len(right)+len(wrong)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
